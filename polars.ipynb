{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "133cb4a3",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/enigma-brain/polars_workshop_2025/blob/main/polars.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c013d964",
   "metadata": {},
   "source": [
    "# Polars Intro Workshop\n",
    "\n",
    "## Learning objectives\n",
    "- why is Enigma using Polars now? \n",
    "- understand advantages of Polars, basics of syntax, what's possible\n",
    "\n",
    "## Why Polars\n",
    "\n",
    "We're switching data stacks: MySQL + DataJoint -> Delta Lake tables + Polars. Several tools can analyze Delta Lake tables, but Polars offers a good balance of [performance](https://pola.rs/posts/benchmarks/) and simplicity.\n",
    "\n",
    "Generally useful if you're working with 2D tables, but also support for lists/arrays as entires.\n",
    "\n",
    "Gaining [popularity](https://www.linkedin.com/posts/jeroenjanssens_polars-just-passed-30k-stars-on-github-congratulations-activity-7256626437082746880-fguS/).\n",
    "\n",
    "Orcapod uses Polars.\n",
    "\n",
    "## Special features of Polars\n",
    "\n",
    "#### Performance and memory\n",
    "- Columnar architecture\n",
    "    - stores and processes data by column rather than by row\n",
    "- Parallelism by default \n",
    "- LazyFrame\n",
    "    - queries aren't immediately executed; Polars optimizes the entire plan first\n",
    "- Streaming\n",
    "    - process datasets larger than available RAM by working in chunks\n",
    "    - can enhance speed of queries\n",
    "\n",
    "\n",
    "## Data I/O\n",
    "- python objects: dict of lists, Pandas DataFrame\n",
    "- CSV, Excel, Parquet\n",
    "- Databases\n",
    "- Delta Lake tables\n",
    "- Cloud storage\n",
    "- others ([Docs](https://docs.pola.rs/user-guide/io/), [API reference](https://docs.pola.rs/api/python/dev/reference/io.html))\n",
    "\n",
    "## Core features for data manipulation, exploratory analysis\n",
    "\n",
    "Understanding these is most of the work. Extending to Lazy API and streaming is easy.\n",
    "\n",
    "#### Single table\n",
    "- `filter` and `select`: filter rows and select columns\n",
    "- `with_columns`: create new columns based on values in other columns\n",
    "- `group_by` and `agg`: group data and compute stats\n",
    "- `pivot` and `unpivot`: convert between long and wide format\n",
    "\n",
    "#### Multi table\n",
    "- `join` and `concat`: join and concatenation\n",
    "\n",
    "## Limitations\n",
    "- newer: smaller community, fewer tutorials\n",
    "- it's compared to Pandas, but the syntax is pretty different (although quite clear)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9332bf19",
   "metadata": {},
   "source": [
    "# Polars demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230bcc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install palmerpenguins\n",
    "import palmerpenguins as pp\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import deltalake as dl\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d955b1d",
   "metadata": {},
   "source": [
    "## I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from dict of lists\n",
    "df = pl.DataFrame(\n",
    "    {\n",
    "        \"nrs\": [1, 2, 3, None, 5],\n",
    "        \"names\": [\"foo\", \"ham\", \"spam\", \"egg\", \"spam\"],\n",
    "        \"random\": np.random.rand(5),\n",
    "        \"groups\": [\"A\", \"A\", \"B\", \"A\", \"B\"],\n",
    "    }\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91712f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV\n",
    "df_csv = pl.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv\")\n",
    "df_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8560f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Parquet\n",
    "df_parquet = pl.read_parquet(\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\")\n",
    "df_parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7670a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Delta Lake Table\n",
    "df_parquet.write_delta(\"/tmp/taxi\")\n",
    "df_from_delta = pl.read_delta(\"/tmp/taxi\")\n",
    "df_from_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a088e93",
   "metadata": {},
   "source": [
    "Each of these functions also has a `scan_*` variant that delays loading/parsing and returns a LazyFrame (described below) instead of a DataFrame.\n",
    "\n",
    "If you `scan` a parquet file stored on the cloud, you can [significantly reduce](https://docs.pola.rs/user-guide/io/parquet/#write:~:text=When%20we%20scan%20a%20Parquet%20file%20stored%20in%20the%20cloud%2C%20we%20can%20also%20apply%20predicate%20and%20projection%20pushdowns.%20This%20can%20significantly%20reduce%20the%20amount%20of%20data%20that%20needs%20to%20be%20downloaded.%20For%20scanning%20a%20Parquet%20file%20in%20the%20cloud%2C%20see%20Cloud%20storage.) how much data you download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767f5139",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"\n",
    "\n",
    "def time_data_load(query):\n",
    "    start = time.time()\n",
    "    query.collect() # data is loaded/processed here\n",
    "    return time.time() - start\n",
    "\n",
    "# This syntax will make more sense later!\n",
    "q1 = pl.scan_parquet(url) # Load ALL columns\n",
    "q2 = pl.scan_parquet(url).select('VendorID') # Load one column\n",
    "q3 = pl.scan_parquet(url).select('VendorID').filter(pl.col('VendorID')==6) # Load one column and filter rows\n",
    "\n",
    "print(f\"All columns: {time_data_load(q1):.2f}s\")\n",
    "print(f\"One column: {time_data_load(q2):.2f}s\") \n",
    "print(f\"One column filtered: {time_data_load(q3):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ca9a64",
   "metadata": {},
   "source": [
    "# Explore a dataset on penguins from the Palmer Archipelago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8b26f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Config.set_fmt_str_lengths(100) # nice for displaying longer strings and not truncate them\n",
    "\n",
    "df = pl.from_pandas(pp.load_penguins())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cb4a8f",
   "metadata": {},
   "source": [
    "## Get basic understanding of data\n",
    "- `head`, `tail`\n",
    "- `glimpse`\n",
    "- `sample`\n",
    "- `schema`\n",
    "- `describe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c44c1de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23ddf56a",
   "metadata": {},
   "source": [
    "# Data manipulation/querying\n",
    "\n",
    "Assuming we have a dataframe called `df`, using Polars generally involves the format:\n",
    "```\n",
    "df.context(expression)\n",
    "```\n",
    "where the 'context' can e.g. be one of the following:\n",
    "\n",
    "- `select` columns\n",
    "- `with_columns` to add columns to data frame\n",
    "- `filter` rows\n",
    "- `group_by` and `agg` to group rows by their values, compute statistics\n",
    "\n",
    "## `select` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cb0e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab a column\n",
    "df.select(\"body_mass_g\")\n",
    "\n",
    "# use pl.col to explicitly create an expression that represents a column\n",
    "df.select(pl.col(\"body_mass_g\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f7f281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can extend pl.col to sort and do arithmetic, \n",
    "df.select(pl.col(\"body_mass_g\").sort() / 1000)\n",
    "\n",
    "# but further operations require more parentheses, e.g. renaming the column\n",
    "df.select(\n",
    "    (pl.col(\"body_mass_g\")\n",
    "     .sort() \n",
    "     / 1000)\n",
    "     .alias(\"body_mass_kg\")\n",
    ") \n",
    "\n",
    "# rename using a 'named expression'\n",
    "df.select(\n",
    "    body_mass_kg = (pl.col(\"body_mass_g\")\n",
    "     .sort() \n",
    "     / 1000)\n",
    ") \n",
    "\n",
    "# alias method nice for storing expressions in variables  \n",
    "kg_expr = ((pl.col(\"body_mass_g\").sort() / 1000).alias(\"body_mass_kg\"))\n",
    "df.select(kg_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985fd705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select multiple columns\n",
    "df.select(\n",
    "    (pl.col(\"species\"), pl.col(\"island\")) # or \"species\", \"island\"\n",
    ").unique().sort(by=\"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae47119e",
   "metadata": {},
   "source": [
    "There are tons of methods to use within expressions (like `.sort`) that can be used in expressions. See [here](https://docs.pola.rs/api/python/stable/reference/expressions/index.html) for a full list.\n",
    "\n",
    "Problem: using `select`, create a column that computes normalized deviations from the mean body mass, and name this column \"body_mass_zscore\". \n",
    "- Hint: .mean() and .std() are probably useful, and broadcasting is supported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa7d3db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7f0eae8",
   "metadata": {},
   "source": [
    "## `with_columns` to add columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b88dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store expressions for two columns to add\n",
    "# not necessary but can enhance readability\n",
    "kg_expr = (pl.col(\"body_mass_g\") / 1000).alias(\"body_mass_kg\")\n",
    "swim_score_expr = (pl.col(\"flipper_length_mm\") / pl.col(\"body_mass_kg\")).alias(\"swim_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5dcf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars claims that expressions in the same context are executed in parallel, using all available CPUs.\n",
    "# If true, this should fail:\n",
    "df.with_columns(\n",
    "    kg_expr,\n",
    "    swim_score_expr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb1eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These columns must be created in serial, by using `with_columns` context twice\n",
    "df = df.with_columns(\n",
    "    kg_expr,\n",
    ").with_columns(\n",
    "    swim_score_expr,\n",
    ")\n",
    "\n",
    "# note we overwrote the original dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02295d4",
   "metadata": {},
   "source": [
    "## `filter` rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b0f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(pl.col(\"species\") == \"Gentoo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019cfbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter based on multiple conditions\n",
    "df.filter((pl.col(\"species\") == \"Adelie\") & (pl.col(\"island\") == \"Biscoe\"))\n",
    "\n",
    "## can use commas instead of ampersands, but not 'and'!\n",
    "df.filter(\n",
    "    (pl.col(\"species\") == \"Adelie\"), \n",
    "    (pl.col(\"island\") == \"Biscoe\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd057f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since expressions can be stored in variables, you can also define functions to create expressions\n",
    "def filter_species_island_expr(species, island):\n",
    "    return (\n",
    "        pl.col(\"species\") == species,\n",
    "        pl.col(\"island\") == island\n",
    "    )\n",
    "\n",
    "df.filter(\n",
    "    filter_species_island_expr(\"Adelie\", \"Biscoe\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa73aa4",
   "metadata": {},
   "source": [
    "Contexts can be chained together using:\n",
    "```\n",
    "df.context1(expression1).context2(expression2)\n",
    "```\n",
    "\n",
    "Problem 1: Find the Chinstrap penguin with the largest swim_score. Return just the swim_score\n",
    "- Hint: think of what you need, and chances are there is an intuitively named method for it, but you can double check [here](https://docs.pola.rs/api/python/stable/reference/expressions/index.html).\n",
    "\n",
    "Problem 2: Find the sex of the Chinstrap penguin with the largest swim score. Return just the sex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55393564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01311346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec4ffc2c",
   "metadata": {},
   "source": [
    "## `groupby` and `agg` to group rows by their values, compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9ed4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mean body mass and swim scores per species\n",
    "df.group_by(\"species\").agg(\n",
    "    pl.col(\"body_mass_kg\").mean(),\n",
    "    pl.col(\"swim_score\").mean(),\n",
    ").sort(by=\"species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28983b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there sex-specific differences? Let's also rename columns\n",
    "df.group_by([\"species\", \"sex\"]).agg(\n",
    "    pl.col(\"body_mass_kg\").mean().alias(\"mean_body_mass_g\"),\n",
    "    pl.col(\"swim_score\").mean().alias(\"mean_swim_score\"),\n",
    ").sort(by=\"species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11ecc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nulls, also count the number of rows in each group\n",
    "(\n",
    "    df\n",
    "    .drop_nulls(subset = [\"sex\"])\n",
    "    .group_by([\"species\", \"sex\"])\n",
    "    .agg(\n",
    "        pl.col(\"body_mass_kg\").mean().alias(\"mean_body_mass_kg\"),\n",
    "        pl.col(\"swim_score\").mean().alias(\"mean_swim_score\"),\n",
    "        pl.len().alias(\"count\") # count rows\n",
    "    )\n",
    "    .sort(by=\"species\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29e884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above, but 'expression expansions' helps remove redundant code\n",
    "(\n",
    "    df\n",
    "    .drop_nulls(subset = [\"sex\"])\n",
    "    .group_by([\"species\", \"sex\"])\n",
    "    .agg(\n",
    "        pl.col(\"body_mass_kg\", \"swim_score\").mean().name.prefix(\"mean_\"), # can also use .name.suffix(\"_mean\")\n",
    "        pl.len().alias(\"count\")\n",
    "    )\n",
    "    .sort(by=\"species\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82d2301",
   "metadata": {},
   "source": [
    "Problem: use the .quantile method to compute the 0.25, 0.5, and 0.75 quantile per species. Name these columns \"q1_swim_score\", \"q2_swim_score\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd4e41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "345d52f2",
   "metadata": {},
   "source": [
    "# Pivots: wide format <-> long format\n",
    "\n",
    "\"Wide\" tables have a column for each variable. \"Long\" tables have fewer columns, with a single column for variable type and another for the corresponding value.\n",
    "\n",
    "- `.pivot()` : long -> wide\n",
    "- `.unpivot()` : wide -> long\n",
    "\n",
    "\n",
    "\n",
    "Our dataframe is already in wide format. Let's convert it to long format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb91ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go from wide to long format\n",
    "df_long = (\n",
    "            df.select(\"species\", \"island\", \"bill_length_mm\", \"bill_depth_mm\")\n",
    "            .unpivot(on=[\"bill_length_mm\", \"bill_depth_mm\"], # columns to collapse into \"variable\" column\n",
    "                    index=[\"species\", \"island\"]) # used as identifier variables\n",
    ")\n",
    "\n",
    "df_long.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e408bbcf",
   "metadata": {},
   "source": [
    "Once in long format, plotting libraries like seaborn make it easy to visualize statistics with respect to these variables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1092bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.violinplot(data=df_long, x=\"species\", y=\"value\", hue=\"variable\")\n",
    "plt.title(\"Distribution of Bill Measurements by Species\")\n",
    "plt.xlabel(\"Species\")\n",
    "plt.ylabel(\"Measurement Value\")\n",
    "plt.legend(title=\"Measurement Type\")\n",
    "plt.show()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd392870",
   "metadata": {},
   "source": [
    "Going from wide to long format involves creating one-to-many relationships: the column name is now represented many times in the variable column.\n",
    "\n",
    "Alternatively, going from long to wide format involves creating many-to-one relationships. As a result, you need to also specify an 'aggregate_function' to tell Polars what to do with duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0807c7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot(on=\"species\", # will become the column values\n",
    "         index=\"sex\", # will become the row values\n",
    "         values=\"body_mass_kg\", # will become the values in the pivot table \n",
    "         aggregate_function=\"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd97aa06",
   "metadata": {},
   "source": [
    "Problem: confirm with command we learned above that the max body_mass_kg for a male Adelie is that value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c90af66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02678255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert back to wide format, but this fails becaue you don't know which bill length+depth go together for each individual\n",
    "df_long.pivot(on=\"variable\",\n",
    "            index=[\"species\", \"island\"],\n",
    "            values=\"value\",\n",
    "            aggregate_function=\"first\") # looking at the options available, this seems most appropriate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2816c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo, preserving penguin ID\n",
    "# go from wide to long format\n",
    "df_long2 = (df\n",
    "            .select(\"species\", \"island\", \"bill_length_mm\", \"bill_depth_mm\")\n",
    "            .with_row_index(\"penguin_id\")\n",
    "            .unpivot([\"bill_length_mm\", \"bill_depth_mm\"],\n",
    "                index=[\"species\", \"island\", \"penguin_id\"])\n",
    ")\n",
    "df_long2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae6e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can successfully go back to wide format\n",
    "df_long2.pivot(on=\"variable\",\n",
    "            index=[\"species\", \"island\", \"penguin_id\"],\n",
    "            values=\"value\",\n",
    "            aggregate_function=\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba1f6d",
   "metadata": {},
   "source": [
    "# Lazy frames\n",
    "For exploratory data analyses, use DataFrames. For performance, use LazyFrames.\n",
    "\n",
    "Syntax same as before, but results not immediately available. Need to use `.collect()`.\n",
    "\n",
    "To load in a LazyFrame, either convert an existing DataFrame or use one of the `.scan_*()` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cfca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.from_pandas(pp.load_penguins())\n",
    "\n",
    "df_lazy = pl.LazyFrame(df) # or df.lazy()\n",
    "\n",
    "df_lazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb8842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lazy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18f69b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_expr = (pl.col(\"body_mass_g\") / 1000).alias(\"body_mass_kg\")\n",
    "gentoo_female_expr = (pl.col(\"species\") == \"Gentoo\") , (pl.col(\"sex\") == \"female\")\n",
    "\n",
    "# make lazy query that gets the body mass in kg for Gentoo males\n",
    "lazy_query = (\n",
    "    df_lazy.with_columns(kg_expr)\n",
    "    .filter(gentoo_female_expr)\n",
    "    .select(\"body_mass_kg\")\n",
    ")\n",
    "\n",
    "# returns another lazy frame\n",
    "lazy_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488701ea",
   "metadata": {},
   "source": [
    "This lazy query, which hasn't been optimized, is read from the bottom up. \n",
    "\n",
    "$\\pi$ stands for PROJECTION and refers to columns. We start with all 10 columns and end up with 1. \n",
    "\n",
    "Contexts are listed in the same order as written in our query.\n",
    "\n",
    "Let's compare this to the optimized query plan below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a0a4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_query.show_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7669bc5f",
   "metadata": {},
   "source": [
    "Polars uses predicate and projection pushdown, database optimization techniques, to make the selection of columns (projection) and the filtering of rows (predicate) as early as possible so that it only loads the data you need.\n",
    "\n",
    "You can also use `.explain()` to get a string representation of the query plan (useful if text gets truncated within graph nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b73e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_query.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a40434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a regular polars data frame, not a lazy frame\n",
    "query_results = lazy_query.collect()\n",
    "print(type(lazy_query))\n",
    "print(type(query_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d737dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f6521e",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "\n",
    "Default is to analyze all the data in memory. For streaming the data in batches, just specify as an argument in .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec86b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_query.collect(engine=\"streaming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25517df2",
   "metadata": {},
   "source": [
    "That's it! Pre-optimizing your queries for speed and streaming the data require little extra syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca429cb",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "- Simple syntax, great for small and large datasets\n",
    "- a handful of 'contexts' will get you a long way:\n",
    "    - select, filter, with_columns, pivot/unpivot, group_by + agg\n",
    "    - join, concat\n",
    "- within a 'context', 'expressions' operate on columns (pl.col) and have many, chainable methods for complex queries\n",
    "- efficient data loading with parquet files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9662d4",
   "metadata": {},
   "source": [
    "# Misc Advanced Stuff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e0a70a",
   "metadata": {},
   "source": [
    "## Reducing memory footprint by recasting types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba6963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# throws error if some values too big for recasting\n",
    "result = df.with_columns(\n",
    "                pl.col(\"year\").cast(pl.Int16), # was Int64\n",
    "                pl.col(\"bill_length_mm\", \n",
    "                       \"bill_depth_mm\", \n",
    "                       \"flipper_length_mm\", \n",
    "                       \"body_mass_g\").cast(pl.Float32)) # was Float64\n",
    "\n",
    "print(f\"Original df: {df.estimated_size()} bytes\")\n",
    "print(f\"Recasted df: {result.estimated_size()} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a892534c",
   "metadata": {},
   "source": [
    "## Loading multiple MySQL tables into Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6851b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install connectorx\n",
    "from urllib.parse import quote\n",
    "from getpass import getpass\n",
    "\n",
    "user = \"barnold\"\n",
    "pswd = quote(getpass('Database password: ')) # Prompt user for password; don't store in code. Use quote() to encode special characters in passwords\n",
    "server = \"at-database3.stanford.edu\"\n",
    "port = 3306\n",
    "database = \"enigma_acq\"\n",
    "tables = [\"sessions\", \"stimulation\", \"behavior_traces\"]\n",
    "\n",
    "\n",
    "uri = f\"mysql://{user}:{pswd}@{server}:{port}/{database}\" \n",
    "dfs = {}\n",
    "for table in tables:\n",
    "    query = f\"SELECT * FROM {table}\"\n",
    "    dfs[table] = pl.read_database_uri(query=query, uri=uri)\n",
    "\n",
    "dfs['sessions']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f34d09",
   "metadata": {},
   "source": [
    "## conditional assignment with window function\n",
    "\n",
    "When multiple .when().then() statementes are used, Polars only considers a replacement expression that is deeper in the chain if the previous ones (predicates) all failed for that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dfe139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accomplishing this in separate pieces\n",
    "\n",
    "quantiles = (\n",
    "   df.group_by([\"species\"]).agg(\n",
    "        pl.col(\"swim_score\").quantile(0.25).alias(\"q1_swim_score\"),\n",
    "        pl.col(\"swim_score\").quantile(0.5).alias(\"q2_swim_score\"),\n",
    "        pl.col(\"swim_score\").quantile(0.75).alias(\"q3_swim_score\"),\n",
    "    )\n",
    ")\n",
    "df_w_quant = df.join(quantiles, on=\"species\")\n",
    "df_w_quant = df_w_quant.with_columns(\n",
    "    pl.when(pl.col(\"swim_score\") <= pl.col(\"q1_swim_score\"))\n",
    "    .then(pl.lit(\"slow\")) # need pl.lit() otherwise Polars treats it as a column\n",
    "    .when(pl.col(\"swim_score\") <= pl.col(\"q2_swim_score\"))\n",
    "    .then(pl.lit(\"intermediate\"))\n",
    "    .when(pl.col(\"swim_score\") <= pl.col(\"q3_swim_score\"))\n",
    "    .then(pl.lit(\"fast\"))\n",
    "    .otherwise(pl.lit(\"really_fast\"))\n",
    "    .alias(\"swim_score_category\")\n",
    ").select(\n",
    "    \"species\", \"swim_score\", \"swim_score_category\"\n",
    ").sort(by=\"species\")\n",
    "\n",
    "df_w_quant.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b05c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accomplishing this in a single step, using windows functions\n",
    "df_categorized = df.with_columns(\n",
    "    pl.col(\"swim_score\").quantile(0.25).over(\"species\").alias(\"q1\"),\n",
    "    pl.col(\"swim_score\").quantile(0.5).over(\"species\").alias(\"q2\"), \n",
    "    pl.col(\"swim_score\").quantile(0.75).over(\"species\").alias(\"q3\"),\n",
    ").with_columns(\n",
    "    pl.when(pl.col(\"swim_score\") <= pl.col(\"q1\"))\n",
    "    .then(pl.lit(\"slow\"))\n",
    "    .when(pl.col(\"swim_score\") <= pl.col(\"q2\"))\n",
    "    .then(pl.lit(\"intermediate\"))\n",
    "    .when(pl.col(\"swim_score\") <= pl.col(\"q3\"))\n",
    "    .then(pl.lit(\"fast\"))\n",
    "    .otherwise(pl.lit(\"really_fast\"))\n",
    "    .alias(\"swim_score_category\")\n",
    ").drop([\"q1\", \"q2\", \"q3\"])  # Remove temporary quantile columns\n",
    "df_categorized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enigma-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
